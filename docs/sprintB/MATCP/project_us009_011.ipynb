{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa56b3a63b86ac8",
   "metadata": {},
   "source": [
    "# US009 \n",
    "\n",
    "## · Introduction\n",
    "\n",
    "###### This US deals with the analysis of water consumption in green spaces, using consumption data provided in a CSV file named \"water_consumption\". The objective of this analysis is to examine water consumption patterns over time and across different parks,calculates the total consumption and associated cost for each park, generates bar plots to visualize monthly water consumption, calculates statistics for parks with the highest and lowest consumption and identify possible outliers in the data. These insights are crucial for efficient water resource management and also for ensuring the environmental sustainability of green spaces.\n",
    "\n",
    "\n",
    "\n",
    "## · Code and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'water_consumption.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m##Reading file \"water_consumption.csv\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(PATH_FILE, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumption\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumption\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumption\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumption\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'water_consumption.csv'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Variables\n",
    "# PATH_FILE = input('Enter the path of the file (water consumption.csv): ')\n",
    "\n",
    "PATH_FILE = \"water_consumption.csv\"\n",
    "WTPRC_1000M3 = 0.7\n",
    "WTCONSUPTIONUPTO = 1000\n",
    "WTPRC_FEE = 0.15\n",
    "FIRST_MONTH = 1\n",
    "LAST_MONTH = 12\n",
    "CLASSES10 = 10\n",
    "CLASSES100 = 100\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "##Reading file \"water_consumption.csv\"\n",
    "data = pd.read_csv(PATH_FILE, sep=\";\")\n",
    "data['Consumption'] = data['Consumption'].str.replace(',', '.')\n",
    "data['Consumption'] = pd.to_numeric(data['Consumption'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "#WaterCost by month and park\n",
    "\n",
    "def calculate_cost(Consumption):\n",
    "    if Consumption <= WTCONSUPTIONUPTO:\n",
    "        return Consumption * WTPRC_1000M3\n",
    "    else:\n",
    "        return (WTCONSUPTIONUPTO * WTPRC_1000M3) + ((Consumption - WTCONSUPTIONUPTO) * WTPRC_1000M3 * WTPRC_FEE)\n",
    "\n",
    "data['Total_Consumption'] = data.groupby(['Park', 'Year', 'Month'])['Consumption'].transform('sum')\n",
    "consumptionInfo = data.groupby(['Park', 'Year', 'Month'])['Total_Consumption'].unique().explode().reset_index(name='Total_Consumption')\n",
    "consumptionInfo['Cost'] = consumptionInfo['Total_Consumption'].apply(calculate_cost)\n",
    "cost_summary = consumptionInfo.loc[:, [\"Park\", \"Year\", \"Month\", \"Cost\"]]\n",
    "print(\"· The cost of consumption of water by each park:\\n\")\n",
    "print(cost_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed4e349bda91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"· I ) Barplot representing monthly water consumption\\n\")\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "years = data[\"Year\"].unique()\n",
    "parks = data[\"Park\"].str.lower().unique()\n",
    "\n",
    "year = int(input(\"Enter the year: \"))\n",
    "text = str(years)\n",
    "while year not in years:\n",
    "    year = int(input(f\"Enter a existent year {text}: \"))\n",
    "\n",
    "start_month = int(input(\"Enter the start month: \"))   \n",
    "while  start_month < FIRST_MONTH or start_month > LAST_MONTH: \n",
    "    start_month = int(input(\"INVALID: Enter a valid start month: \"))\n",
    "    \n",
    "    \n",
    "end_month = int(input(\"Enter the end month: \"))\n",
    "while end_month < FIRST_MONTH or end_month > LAST_MONTH:\n",
    "    end_month = int(input(\"INVALID: Enter a valid end month: \"))\n",
    "    \n",
    "while start_month > end_month:\n",
    "    start_month = int(input(f\"INVALID:Start month > End month ({start_month} > {end_month})\\nEnter a valid start month: \"))\n",
    "\n",
    "\n",
    "parkId = input(\"Enter the Park name: \").lower()\n",
    "while parkId not in parks:\n",
    "    parkId = input(\"INVALID:Enter a existent Park id: \").lower()\n",
    "\n",
    "\n",
    "filtered_data = consumptionInfo[(consumptionInfo['Park'].str.lower() == parkId) & \n",
    "                                (consumptionInfo['Year'] == year) & \n",
    "                                (consumptionInfo['Month'] >= start_month) & \n",
    "                                (consumptionInfo['Month'] <= end_month)]\n",
    "\n",
    "sns.barplot(x=\"Month\", y=\"Total_Consumption\", data=filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591a0defabf64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"· II ) Average Monthly Cost for Each Specified Park:\\n\")\n",
    "###########################################\n",
    "\n",
    "numberOfParks = int(input(\"Enter the number of Parks to see average monthly cost: \"))\n",
    "while numberOfParks > len(parks) or numberOfParks < 0:\n",
    "    numberOfParks = int(input(f\"Enter a valid number of Parks (existent parks {len(parks):.0f}): \"))\n",
    "\n",
    "if numberOfParks  != 0: \n",
    "    idParksAverage = []\n",
    "    for i in range(numberOfParks):\n",
    "        id = input(f\"Enter the Park name (nº:{i+1}) to see average monthly cost: \")\n",
    "        while id not in parks:\n",
    "            id = input(f\"Enter a existent Park (nº:{i+1}): \")\n",
    "        while idParksAverage.__contains__(id) or id not in parks:\n",
    "            id = input(f\"Duplicate park, enter a new one (nº:{i+1}): \")\n",
    "        idParksAverage.append(id)\n",
    "    \n",
    "    # Calculate average monthly cost for specified parks\n",
    "    average_cost_per_park = cost_summary.groupby('Park')['Cost'].mean()\n",
    "    \n",
    "    # Display average monthly cost for specified parks\n",
    "    for park_id in idParksAverage:\n",
    "        if park_id in average_cost_per_park.index:\n",
    "            print(f\"Park ID: {park_id}\\n ·Average Monthly Cost: {average_cost_per_park[park_id]:.2f}€\")\n",
    "else:\n",
    "    print(\"ATTENTION: 0 parks selected, no average monthly cost\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afceef246e042ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"· III )\")\n",
    "###########################################\n",
    "\n",
    "# Statistics\n",
    "print(\"·· 1) Calculate statistics for the park/s with the highest and lowest consumption.\\n\")\n",
    "\n",
    "##HIGHEST\n",
    "print(\"[ Highest consumption park/s ]\")\n",
    "print(\"  --------------------------\\n\")\n",
    "highest_consumption_park = data.groupby('Park')['Consumption'].max()\n",
    "maxValue = highest_consumption_park.max()\n",
    "\n",
    "for park_id, value in highest_consumption_park.items():\n",
    "    if value == maxValue:\n",
    "        park_with_highest_consumption = park_id\n",
    "        # Calculate statistics for the park with the highest consumption\n",
    "        highest_consumption_stats = data[data['Park'] == park_with_highest_consumption]['Consumption'].agg(['mean', 'median', 'std', 'skew'])\n",
    "        highest_consumption_stats = highest_consumption_stats.rename(lambda x: f'{park_with_highest_consumption}_{x}')\n",
    "        print(f\"Statistics for the park ({park_with_highest_consumption} - {highest_consumption_park.max():.2f} m3):\")\n",
    "        print(highest_consumption_stats.to_string())\n",
    "        print()\n",
    "\n",
    "\n",
    "##lOWEST\n",
    "print(\"[ Lowest consumption park/s ]\")\n",
    "print(\"  -------------------------\\n\")\n",
    "\n",
    "def min_consumption_without_zero(group):\n",
    "    non_zero_values = group[group != 0]  # Exclude zero values\n",
    "    if non_zero_values.empty:\n",
    "        return 0  # Return 0 if all values are zero\n",
    "    else:\n",
    "        return non_zero_values.min()\n",
    "\n",
    "\n",
    "lowest_consumption_park = data.groupby('Park')['Consumption'].apply(min_consumption_without_zero)\n",
    "minValue = lowest_consumption_park.min()\n",
    "\n",
    "for park_id, value in lowest_consumption_park.items():\n",
    "    if value == minValue and (not maxValue == highest_consumption_park[park_id].max()):\n",
    "        park_with_lowest_consumption = park_id\n",
    "        # Calculate statistics for the park with the lowest consumption\n",
    "        lowest_consumption_stats = data[data['Park'] == park_with_lowest_consumption]['Consumption'].agg(['mean', 'median', 'std', 'skew'])\n",
    "        lowest_consumption_stats = lowest_consumption_stats.rename(lambda x: f'{park_with_lowest_consumption}_{x}')\n",
    "        print(f\"Statistics for the park ({park_with_lowest_consumption} - {lowest_consumption_park.min():.2f} m3):\")\n",
    "        print(lowest_consumption_stats.to_string())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1a9ff24f84c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"· III )\")\n",
    "###########################################\n",
    "\n",
    "print(\"·· 2) Build relative and absolute frequency tables.\\n\")\n",
    "C = 5\n",
    "\n",
    "##HIGHEST\n",
    "\n",
    "\n",
    "print(\"[ Highest consumption park/s ]\")\n",
    "print(\"  --------------------------\\n\\n\")\n",
    "for park_id, value in highest_consumption_park.items():\n",
    "    if value == maxValue:\n",
    "        park_with_highest_consumption = park_id\n",
    "        park_with_highest_consumption_data = data[data['Park'] == park_with_highest_consumption]['Consumption']\n",
    "        \n",
    "        n_highest = len(park_with_highest_consumption_data)\n",
    "        data_min_highest = park_with_highest_consumption_data.min()\n",
    "        data_max_highest = park_with_highest_consumption_data.max()\n",
    "        \n",
    "        # Divide em 5 classes\n",
    "        class_interval_highest = (data_max_highest - data_min_highest) / C \n",
    "         # Limite inferior e superior de cada classe\n",
    "        class_limits_highest = [data_min_highest + i * class_interval_highest for i in range(C+1)] \n",
    "        #Intervalos\n",
    "        classes_highest = [[round(class_limits_highest[i],2) ,round(class_limits_highest[i+1],2)] for i in range(C)]\n",
    "        \n",
    "        #FQ absoluta\n",
    "        absolute_frequencies_highest = [sum(1 for value in park_with_highest_consumption_data if class_limits_highest[i] <= value < class_limits_highest[i+1]) for i in range(C)]\n",
    "        #FQ relativa\n",
    "        relative_frequencies_highest = [round((freq / n_highest) *100,2) for freq in absolute_frequencies_highest] \n",
    "        #FQ acumulada\n",
    "        acumulate_frequencies_highest = []\n",
    "        soma = 0\n",
    "        for i in absolute_frequencies_highest:\n",
    "            soma += i\n",
    "            acumulate_frequencies_highest.append(soma)\n",
    "        \n",
    "        dados = {\n",
    "            \"Classes\": classes_highest,\n",
    "            \"Freq_abs\": absolute_frequencies_highest,\n",
    "            \"Freq_rel(%)\": relative_frequencies_highest,\n",
    "            \"Freq_acum\": acumulate_frequencies_highest\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(dados)\n",
    "        print(f\"Park : {park_id}\\n\")\n",
    "        print(df)\n",
    "        print(\"\\n\")\n",
    "  \n",
    "  \n",
    "##lOWEST\n",
    "print(\"[ Lowest consumption park/s ]\")\n",
    "print(\"  -------------------------\\n\\n\")   \n",
    "\n",
    "for park_id, value in lowest_consumption_park.items():\n",
    "    if value == minValue and (not maxValue == highest_consumption_park[park_id].max()):    \n",
    "        park_with_lowest_consumption = park_id\n",
    "        park_with_lowest_consumption_data = data[data['Park'] == park_with_lowest_consumption]['Consumption']\n",
    "        n_lowest = len(park_with_lowest_consumption_data)\n",
    "        \n",
    "        data_min_lowest = park_with_lowest_consumption_data.min()\n",
    "        data_max_lowest = park_with_lowest_consumption_data.max()\n",
    "        # Divide em 5 classes\n",
    "        class_interval_lowest = (data_max_lowest - data_min_lowest) / C  \n",
    "        class_limits_lowest = [data_min_lowest + i * class_interval_lowest for i in range(C+1)]  # Limite inferior e superior de cada classe        \n",
    "        classes_highest = [[round(class_limits_lowest[i],2) ,round(class_limits_lowest[i+1],2)] for i in range(C)]\n",
    "        \n",
    "        #FQ absoluta\n",
    "        absolute_frequencies_highest = [sum(1 for value in park_with_lowest_consumption_data if class_limits_lowest[i] <= value < class_limits_lowest[i+1]) for i in range(C)]\n",
    "        #FQ relativa\n",
    "        relative_frequencies_highest = [round((freq / n_lowest) *100,2) for freq in absolute_frequencies_highest] \n",
    "        #FQ acumulada\n",
    "        acumulate_frequencies_highest = []\n",
    "        soma = 0\n",
    "        for i in absolute_frequencies_highest:\n",
    "            soma += i\n",
    "            acumulate_frequencies_highest.append(soma)\n",
    "        \n",
    "        dados = {\n",
    "            \"Classes\": classes_highest,\n",
    "            \"Freq_abs\": absolute_frequencies_highest,\n",
    "            \"Freq_rel(%)\": relative_frequencies_highest,\n",
    "            \"Freq_acum\": acumulate_frequencies_highest\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(dados)\n",
    "        print(f\"Park : {park_id}\\n\")\n",
    "        print(df)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eace6461876d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"· III )\")\n",
    "###########################################\n",
    "print(\"·· 3) Find outliers.\\n\")\n",
    "\n",
    "\n",
    "# Group the data by each park\n",
    "grouped_data = data.groupby('Park')\n",
    "\n",
    "# Iterate over each group (park)\n",
    "for park, group in grouped_data:\n",
    "    # Calculate median and interquartile range (IQR)\n",
    "    median = group['Consumption'].median()\n",
    "    q1 = group['Consumption'].quantile(0.25)\n",
    "    q3 = group['Consumption'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Define outlier range\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = group[(group['Consumption'] < lower_bound) | (group['Consumption'] > upper_bound)]\n",
    "    \n",
    "    # Print results\n",
    "    if not outliers.empty:\n",
    "        print(f\"Park ID: {park}:\\n\")\n",
    "        print(outliers)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No outliers found for {park}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ed9d36e40dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"· III )\")\n",
    "###########################################\n",
    "print(\"·· 4) Histograms\\n\")\n",
    "\n",
    "for park_id, value in highest_consumption_park.items():\n",
    "    if value == maxValue: \n",
    "        consumption_park = data[data['Park'] == park_id]['Consumption']\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(consumption_park, bins=CLASSES10, color='skyblue', alpha=0.8)\n",
    "        plt.title(f'Histograma com 10 Classes : Highest Value\\n Park : {park_id}')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(consumption_park, bins=CLASSES100, color='skyblue', alpha=0.8)\n",
    "        plt.title(f'Histograma com 100 Classes : Highest Value\\n Park : {park_id}')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        \n",
    "for park_id, value in lowest_consumption_park.items():\n",
    "    if value == minValue and (not maxValue == highest_consumption_park[park_id].max()): \n",
    "        consumption_park = data[data['Park'] == park_id]['Consumption']\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(consumption_park, bins=CLASSES10, color='skyblue', alpha=0.8)\n",
    "        plt.title(f'Histograma com 10 Classes : Lowest Value\\n Park : {park_id}')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(consumption_park, bins=CLASSES100, color='skyblue', alpha=0.8)\n",
    "        plt.title(f'Histograma com 100 Classes : Lowest Value\\n Park : {park_id}')\n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e9613082d65d0",
   "metadata": {},
   "source": [
    "## · Formulas (Latex)\n",
    "\n",
    "$$\n",
    "\\text{Cost of water} = \n",
    "\\begin{cases} \n",
    "\\text{Consumption} \\times 0.7 & \\text{if Consumption} \\leq 1000 \\\\\n",
    "(1000 \\times 0.7) + ((\\text{Consumption} - 1000) \\times 0.7 \\times 0.15) \\text{     } & \\text{if } 1000 \\leq \\text{Consumption}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Median} = \n",
    "\\begin{cases} \n",
    "\\text{Middle value of the sorted dataset    } & \\text{if $n$ is odd} \\\\\n",
    "\\frac{\\text{Sum of the two middle values}}{2   } & \\text{if $n$ is even}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Standard Deviation} = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Coefficient of Skewness} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^3}{n \\times \\sigma^3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Absolute Frequency} = {\\sum_{i=1}^{c} n_i} = n\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Relative Frequency} = f_i = \\frac{{n_i}}{{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "##### Where:\n",
    "\n",
    "$$\\( \\bar{x} \\)\\text{ - } is \\text{ } the \\text{ } mean$$\n",
    "$$\\( \\sigma \\)\\text{ - } is \\text{ } the\\text{ } standard \\text{ }deviation\\text{ }$$\n",
    "$$\\( n \\)\\text{ - } is\\text{ }  the\\text{ }  number\\text{ }  of\\text{ }  data\\text{ }  points$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## · Analysis and Interpretation of the Results:\n",
    "\n",
    "\n",
    "\n",
    "#####   1) Cost of Water Consumption per Park:\n",
    " Analyzing the cost of water consumption per park provides a clear understanding of the expenses associated with maintaining these green spaces. This helps identify which parks have the highest water consumption costs and can guide decisions regarding resource allocation and implementation of conservation measures.\n",
    "\n",
    "#####  2) Monthly Water Consumption Barplot:\n",
    "\n",
    "Bar plots representing monthly water consumption allow visualizing seasonal patterns in water consumption in each park. This can reveal peak consumption months.\n",
    "\n",
    "#####  3) Average Monthly Cost per Park:\n",
    "\n",
    "Calculating the average monthly cost per park offers an overview of the average expenses of each park over time. This assists in identifying cost trends and comparing the performance of different parks in terms of water usage efficiency.\n",
    "\n",
    "#####  4) Consumption Statistics for Parks with Highest and Lowest Consumption:\n",
    "\n",
    "This provides a detailed understanding of the distributions of water consumption in each park. This includes measures of central tendency (mean, median), dispersion (standard deviation), and distribution shape (skewness). These statistics help identify parks with extreme consumption patterns and understand the variability in the data.\n",
    "\n",
    "#####  5) Relative and Absolute Frequency Tables:\n",
    "\n",
    "Relative and absolute frequency tables offer a detailed view of the distribution of water consumption in specific intervals for parks with the highest and lowest consumption. This helps identify the most common consumption intervals and assess the uniformity or disparity in consumption distribution among different parks.\n",
    "\n",
    "##### 6) Outlier Detection:\n",
    "\n",
    "Identifying outliers in water consumption data allows pinpointing unusual observations that may indicate issues such as leaks, measurement errors, or anomalous consumption patterns. This aids in identifying areas requiring further investigation or corrective action.\n",
    "\n",
    "#####  7) Histograms with 10 and 100 Classes:\n",
    "\n",
    "Histograms with different numbers of classes provide a visual representation of the distribution of water consumption in each park. This allows visualizing the shape of the distribution, identifying clustering patterns, and assessing the uniformity of consumption distribution across different intervals.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## · Conclusion\n",
    "\n",
    "\n",
    "This data analysis revealed interesting water consumption patterns in different parks over time. Additionally, outliers were identified in some parks, indicating cases of excessive or unusual consumption. The generated histograms provide a clear visualization of the distribution of water consumption in each park, allowing for comparison across different data classes. These insights are crucial for efficient water resource management, enabling the identification of areas that require greater attention and intervention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e008fbd279cc",
   "metadata": {},
   "source": [
    "### US0010 \n",
    "\n",
    "#### · Introduction\n",
    "This US has the objective of knowing the preferences of the users in the park, for that we must record the usage of each equipment in a csv file and then, display them in a pie chart with the percentage of usage of each piece of equipment.\n",
    "\n",
    "#### · Code and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd13be29af802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file = \"EquipmentUsed.csv\"\n",
    "\n",
    "dataFrame = pd.read_csv(file)\n",
    "\n",
    "equipmentCount = dataFrame.apply(pd.Series.value_counts).sum(axis=1)\n",
    "\n",
    "equipmentPercentage = (equipmentCount / equipmentCount.sum()) * 100\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(equipmentPercentage, labels=equipmentPercentage.index, autopct='%1.1f%%', startangle=0)\n",
    "plt.axis('equal')\n",
    "plt.title('Percentage of Equipment Usage\\n\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0691d00ee0a4",
   "metadata": {},
   "source": [
    "#### · Formulas\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{n} = (\\frac{{u}}{{t}}) * 100\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "##### Where:\n",
    "\n",
    "$$\\( \\text{n} \\)\\text{ - percentage of usage of a equipment } $$\n",
    "\n",
    "$$\\( \\text{u} \\)\\text{ - number of times certain equipment was used } $$\n",
    "\n",
    "$$\\( \\text{t} \\)\\text{ - total number of equipment used}  $$\n",
    "\n",
    "\n",
    "\n",
    "## · Analysis and Interpretation of the Results:\n",
    "This code starts by asking the user to write the name of the csv file to be read. Creates a dataFrame from the data, counts the number of times each equipment was used, and then finishes by generating a pie chart to visualize the percentage of equipment usage.\n",
    "\n",
    "After reading the data from the file using Pandas and a dataFrame, \"dataFrame.apply(pd.Series.value_counts).sum(axis=1)\" counts the number of times each equipment was used and stores it in equipmentCount.\n",
    "\"equipmentPercentage = (equipmentCount / equipmentCount.sum()) * 100\" uses the equipmentCount from earlier and then divides it by the total number of equipments used, finally it multiples by 100 to make the equipmentPercentage.\n",
    "At last, it displays the pie chart using the data that was read using matplotlib.pyplot.\n",
    "\n",
    "Overall, this script allows users to analyze equipment usage data from a csv file and provides a representation of the distribution of equipment usage percentages.\n",
    "The graph that results from this code shows the percentage of usage of each equipment from the list given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3ff286cf0c04c",
   "metadata": {},
   "source": [
    "### US011 \n",
    "\n",
    "#### · Introduction\n",
    "\n",
    "This report presents an analysis of park visitor data extracted from a CSV file. Utilizing Python's pandas and matplotlib libraries, the data is processed to determine variable types and calculate proportions of park visitors recommending the park across different age groups. Additionally, boxplots are generated to visualize the monthly frequency of park use categorized by age groups.\n",
    "\n",
    "\n",
    "#### · Code and results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a58a5a235496bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:39:59.269881Z",
     "start_time": "2024-05-12T21:39:58.989477Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Inquiry.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInquiry.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m child \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m adult \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Inquiry.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"Inquiry.csv\", delimiter=';')\n",
    "\n",
    "child = []\n",
    "adult = []\n",
    "senior = []\n",
    "\n",
    "for index, escalao in enumerate(data['Escalao']):\n",
    "    if escalao == 1 :\n",
    "        child.append(data['Y/N'][index])\n",
    "    elif escalao == 2 :\n",
    "        adult.append(data['Y/N'][index])\n",
    "    elif escalao == 3 :\n",
    "        senior.append(data['Y/N'][index])\n",
    "\n",
    "child = pd.Series(child)\n",
    "adult = pd.Series(adult)\n",
    "senior = pd.Series(senior)\n",
    "\n",
    "child_group_recommendation_proportion = child.eq('Y').mean() if len(child) > 0 else 0.0\n",
    "adult_group_recommendation_proportion = adult.eq('Y').mean() if len(adult) > 0 else 0.0\n",
    "senior_group_recommendation_proportion = senior.eq('Y').mean() if len(senior) > 0 else 0.0\n",
    "\n",
    "print(f\"Escalao - int | Y/N - String | Visits - int\\n\")\n",
    "\n",
    "print(f\"Child group recommendation proportion: {child_group_recommendation_proportion*100:.3}%\")\n",
    "print(f\"Adult group recommendation proportion: {adult_group_recommendation_proportion*100:.3}%\")\n",
    "print(f\"Senior group recommendation proportion: {senior_group_recommendation_proportion*100:.3}%\")\n",
    "\n",
    "child_data = data[data['Escalao'] == 1]\n",
    "adult_data = data[data['Escalao'] == 2]\n",
    "senior_data = data[data['Escalao'] == 3]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([child_data['Visits'], adult_data['Visits'], senior_data['Visits']],\n",
    "            labels=['Child (<= 15)', 'Adult (16-65)', 'Senior (>= 66)'])\n",
    "plt.title('Monthly Visits Frequency by Age Group')\n",
    "plt.xlabel('Age Groups')\n",
    "plt.ylabel('Visits')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7bab0d4cfde38",
   "metadata": {},
   "source": [
    "This formula calculates the mean of recomendations of each age group:\n",
    "$$\n",
    "\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "$$\n",
    "\n",
    "Shows the range of each age group:\n",
    "$$\n",
    "\\text{Age range} = \n",
    "\\begin{cases} \n",
    "\\text{Child} & \\text{if } \\text{Age} \\leq 15 \\\\\n",
    "\\text{Adult} & \\text{if } 16 \\leq \\text{Age} \\leq 65 \\\\\n",
    "\\text{Senior} & \\text{if } \\text{Age} \\geq 66\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "General proportion formula:\n",
    "$$\n",
    "\\text{Proportion} = \\frac{\\text{Number of users who recommend the park}}{\\text{Total number of users in the age group}}\n",
    "$$\n",
    "\n",
    "Proportion formula of each age group:\n",
    "$$\n",
    "\\text{Child Group Proportion} = \\frac{\\text{Number of children recommending the park}}{\\text{Total number of children}}\\\\\n",
    "\\text{Adult Group Proportion} = \\frac{\\text{Number of adults recommending the park}}{\\text{Total number of adults}}\\\\\n",
    "\\text{Senior Group Proportion} = \\frac{\\text{Number of seniors recommending the park}}{\\text{Total number of seniors}}\n",
    "$$\n",
    "\n",
    "#### · Analysis and Interpretation of the Results:\n",
    "The provided code conducts an analysis of a dataset stored in a CSV file named \"Inquiry.csv\", focusing on age groups categorized as Child, Adult, and Senior. It first calculates the proportion of individuals within each age group who received a recommendation. It also graphs the distribution of monthly visit frequencies for each age group, using a boxplot. The analysis highlights the fact that the senior population receives greater numbers of visits, having a major impact on recommendations.\n",
    "\n",
    "\n",
    "Also by analysing the graph we can observe the following topics:\n",
    "\n",
    "**Median (line in the middle of the box):** The median is represented by the line that divides the box in half. It indicates the central value of the data. Being three the median value of Child, five of adult and seven of senior.\n",
    "\n",
    "**Quartiles (box boundaries):** The box of the boxplot is bounded by quartiles. The first quartile (Q1) is the value below which 25% of the data lies, while the third quartile (Q3) is the value below which 75% of the data lies. The difference between Q3 and Q1 is called the interquartile range (IQR), which can be used to measure data dispersion.\n",
    "\n",
    "**Whiskers (lines extending from the box):** The lines extending from the sides oaf the box represent the boundaries of the data not considered outliers. The length of the lines may vary depending on the implementation but usually follows a statistical rule. For example, they might be 1.5 times the IQR. Any value outside of these limits is considered an outlier.\n",
    "\n",
    "**Outliers (points outside the whiskers):** Individual points falling outside the whisker limits are considered outliers. They may indicate extreme values or errors in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd149ce906d4624d",
   "metadata": {},
   "source": [
    "# · Self Evaluation\n",
    "####\n",
    "#### 1230444 (Romeu Xu)            - 20%\n",
    "#### 1230839 (Emmanuel Almeida)    - 20%\n",
    "#### 1230564 (Francisco Santos)    - 20%\n",
    "#### 1231498 (Paulo Mendes)        - 20%\n",
    "#### 1231274 (Jorge Ubaldo)        - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da463dd5ef79af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
